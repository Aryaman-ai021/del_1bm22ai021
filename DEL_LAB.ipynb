{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Lab-6\n"
      ],
      "metadata": {
        "id": "I8T2fbJp76pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc = nn.Linear(28 * 28, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        return self.fc(x)\n",
        "\n",
        "def generate_adversarial(model, data, target, epsilon):\n",
        "    data.requires_grad = True\n",
        "    loss = nn.CrossEntropyLoss()(model(data), target)\n",
        "    loss.backward()\n",
        "    return torch.clamp(data + epsilon * data.grad.sign(), 0, 1)\n",
        "\n",
        "def tangent_prop_loss(model, data, tangent_vectors, target, weight):\n",
        "    output = model(data)\n",
        "    loss = nn.CrossEntropyLoss()(output, target)\n",
        "\n",
        "    tangent_loss = 0\n",
        "    for t in tangent_vectors:\n",
        "        tangent_noise = t.expand_as(data)\n",
        "        tangent_loss += ((model(data + tangent_noise) - output) ** 2).mean()\n",
        "\n",
        "    return loss + weight * tangent_loss\n",
        "\n",
        "def tangent_distance(x1, x2, tangents):\n",
        "    return min(np.linalg.norm(x1 - x2 - t.numpy()) for t in tangents)\n",
        "\n",
        "def tangent_classifier(x, training_data, labels, tangents):\n",
        "    distances = [tangent_distance(x, train_x, tangents) for train_x in training_data]\n",
        "    return labels[np.argmin(distances)]\n",
        "\n",
        "def train(model, train_loader, optimizer, epsilon, tangents, weight):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        adv_data = generate_adversarial(model, data, target, epsilon)\n",
        "        optimizer.zero_grad()\n",
        "        loss = tangent_prop_loss(model, data, tangents, target, weight) + \\\n",
        "               nn.CrossEntropyLoss()(model(adv_data), target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item()}\")\n",
        "\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleNN().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "tangents = [torch.randn(1, 1, 28, 28, device=device) * 0.1 for _ in range(5)]\n",
        "\n",
        "train(model, train_loader, optimizer, epsilon=0.2, tangents=tangents, weight=0.1)"
      ],
      "metadata": {
        "id": "xEEFrrZQCVPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939d776b-e194-42a3-d7cd-a2b7e13f9998"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 6.094879627227783\n",
            "Batch 100: Loss = 3.7800815105438232\n",
            "Batch 200: Loss = 3.0397844314575195\n",
            "Batch 300: Loss = 2.8925375938415527\n",
            "Batch 400: Loss = 2.5339808464050293\n",
            "Batch 500: Loss = 2.877753257751465\n",
            "Batch 600: Loss = 2.5696511268615723\n",
            "Batch 700: Loss = 2.5015130043029785\n",
            "Batch 800: Loss = 2.8095476627349854\n",
            "Batch 900: Loss = 2.5198118686676025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZJ2mdb7F10t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}